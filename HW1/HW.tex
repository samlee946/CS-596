\documentclass[letter, 11pt]{article}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{eqparbox}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}

\author{Shengjie Li}
\title{Homework 1}

\pagestyle{fancy}
\fancyhf{} 
\lhead{Shengjie Li \\ RUID: 188008047}
\cfoot{\thepage} 
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headwidth}{\textwidth}
\renewcommand\algorithmiccomment[1]{%
	\hfill\#\ \eqparbox{COMMENT}{#1}%
}
\newlist{subquestion}{enumerate}{1}
\setlist[subquestion, 1]{label = \alph*)}

\setlength\parindent{0pt}

% margin adjustment
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-0.375in}
\addtolength{\evensidemargin}{-0.375in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1.0in}
\setlength\parindent{0cm}

\begin{document}
	\centerline{Homework 1}
	\begin{enumerate}[wide = 0pt, label = \textbf{Problem \arabic*:}]
		\item {Consider the matrix \[ A = \begin{bmatrix}
			1 & 0.5 \\
			0 & 1 + \epsilon
			\end{bmatrix}. \]} 
		\begin{subquestion}
			\item {Find the eigenvalues/eigenvectors of $ A $ assuming $ \epsilon \ne 0 $. Force your eigenvectors to have unit norm.} 
			\begin{align*}
				A - \lambda I &= \begin{bmatrix}
								1 - \lambda & 0.5 \\
								0 & 1 + \epsilon - \lambda
								\end{bmatrix} \\
				\det(A - \lambda I) &= (1 - \lambda) * (1 + \epsilon - \lambda) - 0.5 * 0 \\
				&= (1 - \lambda) * (1 + \epsilon - \lambda)
			\end{align*}
			Let $ \det(A - \lambda I) = 0 $ we can get eigenvalues $ \lambda_1 = 1, \lambda_2 = 1 + \epsilon $. \\
			For $ \lambda = 1 $, solve $ A \vec{\bm{x}} = \lambda \vec{\bm{x}} $: 
			\begin{align*}
				\begin{bmatrix}
				1 & 0.5 \\
				0 & 1 + \epsilon
				\end{bmatrix}
				\begin{bmatrix}
				x_1 \\ x_2
				\end{bmatrix}
				&= 
				\begin{bmatrix}
				x_1 \\ x_2
				\end{bmatrix}
				\\ \downarrow \\
				x_1 + 0.5x_2 &= x_1 \\
				(1 + \epsilon) x_2 &= x_2 
				\\ \downarrow \\
				x_1 &= x_1 (x_1 \ne 0)\\
				x_2 &= 0
			\end{align*}
			Therefore, $ \begin{bmatrix} 1 \\ 0 \end{bmatrix} $ is a eigenvector of $ A $ associated with the eigenvalue $ \lambda = 1 $. \\
			For $ \lambda = 1 + \epsilon $, solve $ A \vec{\bm{x}} = \lambda \vec{\bm{x}} $: 
			\begin{align*}
				\begin{bmatrix}
				1 & 0.5 \\
				0 & 1 + \epsilon
				\end{bmatrix}
				\begin{bmatrix}
				x_1 \\ x_2
				\end{bmatrix}
				&= 
				(1 + \epsilon)
				\begin{bmatrix}
				x_1 \\ x_2
				\end{bmatrix}
				\\ \downarrow \\
				x_1 + 0.5x_2 &= (1 + \epsilon) x_1 \\
				(1 + \epsilon) x_2 &= (1 + \epsilon) x_2 
				\\ \downarrow \\
				x_1 &= \frac{x_2}{2 \epsilon}\\
				x_2 &= x_2
			\end{align*}
			Therefore, $ \begin{bmatrix} \frac{1}{\sqrt{1 + 4 \epsilon^2}} \\ \frac{2 \epsilon}{\sqrt{1 + 4 \epsilon^2}} \end{bmatrix} $ is a eigenvector of $ A $ associated with the eigenvalue $ \lambda = 1 + \epsilon $. \\
			
			\item {Diagonalize $ A $ using the eigenvalues/eigenvectors you computed.} \\
			Let $ T $ be the matrix with eigenvectors as its columns.
			\begin{align*}
				T &= 
				\begin{bmatrix}
				1 & \frac{1}{\sqrt{1 + 4 \epsilon^2}} \\
				0 & \frac{2 \epsilon}{\sqrt{1 + 4 \epsilon^2}}
				\end{bmatrix}
				\\
				T^{-1} &= \frac{1}{\frac{2 \epsilon}{\sqrt{1 + 4 \epsilon^2}}} 
				\begin{bmatrix}
				\frac{2 \epsilon}{\sqrt{1 + 4 \epsilon^2}} & -\frac{1}{\sqrt{1 + 4 \epsilon^2}} \\
				0 & 1
				\end{bmatrix}
				\\
				&=
				\begin{bmatrix}
				1 & -\frac{1}{2 \epsilon} \\
				0 & \frac{\sqrt{1 + 4 \epsilon^2}}{2 \epsilon}
				\end{bmatrix}
				\\
				\Lambda = T^{-1}AT &= 
				\begin{bmatrix}
				1 & -\frac{1}{2 \epsilon} \\
				0 & \frac{\sqrt{1 + 4 \epsilon^2}}{2 \epsilon}
				\end{bmatrix}
				\begin{bmatrix}
				1 & 0.5 \\
				0 & 1 + \epsilon
				\end{bmatrix}
				\begin{bmatrix}
				1 & \frac{1}{\sqrt{1 + 4 \epsilon^2}} \\
				0 & \frac{2 \epsilon}{\sqrt{1 + 4 \epsilon^2}}
				\end{bmatrix}
				\\
				&=
				\begin{bmatrix}
				1 & -\frac{1}{2 \epsilon}\\
				0 & \frac{(1 + \epsilon)\sqrt{1 + 4 \epsilon^2}}{2 \epsilon}
				\end{bmatrix}
				\begin{bmatrix}
				1 & \frac{1}{\sqrt{1 + 4 \epsilon^2}} \\
				0 & \frac{2 \epsilon}{\sqrt{1 + 4 \epsilon^2}}
				\end{bmatrix}
				\\
				&=
				\begin{bmatrix}
				1 & 0 \\
				0 & 1 + \epsilon
				\end{bmatrix}
			\end{align*} \\
			
			\item {Start now sending $ \epsilon \to 0 $. What do
				you observe is happening to the matrices you use for diagonalization as $ \epsilon $ becomes smaller and smaller? So
				what do you conclude when $ \epsilon = 0 $?} \\
			\\
			When $ \epsilon $ is becoming closer to 0, the determinant of matrix $ T $ is becoming closer to 0. \\
			Thus, when $ \epsilon = 0 $, matrix $ T $ will become non-invertible. Matrix $ A $ will become non-diagonalizable.
		\end{subquestion}
		
		\item {Let A, B be two matrices of the same dimensions k Ã— m.}
		\begin{subquestion}
			\item {With direct computation show that $ trace(AB^T ) = trace(B^T A) = trace(BA^T ) = trace(A^T B) $.} \\ \\
			The $ i-th $ element in the diagonal of matrix $ AB^T $ is $ \sum\limits_{j = 1}^{m} a_{ij}b_{ij}$. Thus, $ trace(AB^T) = \sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{m} a_{ij}b_{ij} $. \\
			The $ j-th $ element in the diagonal of matrix $ B^T A $ is $ \sum\limits_{i = 1}^{k} b_{ij}a_{ij}$. Thus, $ trace(B^T A) = \sum\limits_{j = 1}^{m}\sum\limits_{i = 1}^{k} b_{ij}a_{ij} = \sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{m} a_{ij}b_{ij}$. \\
			The $ i-th $ element in the diagonal of matrix $ BA^T $ is $ \sum\limits_{j = 1}^{m} b_{ij}a_{ij}$. Thus, $ trace(BA^T) = \sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{m} b_{ij}a_{ij} = \sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{m} a_{ij}b_{ij}$.  \\
			The $ j-th $ element in the diagonal of matrix $ A^T B $ is $ \sum\limits_{i = 1}^{k} a_{ij}b_{ij}$. Thus, $ trace(A^T B) = \sum\limits_{j = 1}^{m}\sum\limits_{i = 1}^{k} a_{ij}b_{ij} = \sum\limits_{i = 1}^{k}\sum\limits_{j = 1}^{m} a_{ij}b_{ij}$. \\
			Therefore, $ trace(AB^T ) = trace(B^T A) = trace(BA^T ) = trace(A^T B) $.
			
			\item {Use question a) to compute $ \mathsf{E}[\textbf{x}^T A\textbf{x}] $ where $ \mathsf{E}[\cdot] $ denotes expectation, $ A $ is a constant matrix and $ \textbf{x} $ is a random vector for which we know that $ \mathsf{E}[\textbf{xx}^T ] = Q $. \textit{Hint: The trace of a scalar is the scalar itself.}} 
			\begin{align*}
				\shortintertext{Because $ \textbf{x}^T A\textbf{x} $ is a scalar, Therefore:}
				\textbf{x}^T A\textbf{x} &= trace(\textbf{x}^T A\textbf{x}) \\
				\mathsf{E}[\textbf{x}^T A\textbf{x}] &= \mathsf{E}[trace(\textbf{x}^T A\textbf{x})] \\
				&= \mathsf{E}[trace(A\textbf{x} \textbf{x}^T)] \\
				&= trace(\mathsf{E}[A\textbf{x} \textbf{x}^T]) \\
				\shortintertext{Because A is constant, Therefore:} 
				\mathsf{E}[\textbf{x}^T A\textbf{x}] &= trace(A \cdot \mathsf{E}[\textbf{x} \textbf{x}^T]) \\
				&= trace(A \cdot Q)
			\end{align*}
			
			\item {Using the previous properties show that for any matrix $ A $ of dimensions $ k \times k $ we have $ trace(A) = trace(U AU^{-1}) $ for any nonsingular matrix $ U $ of dimensions $ k \times k $. In other words that the trace does not change if we apply a similarity transformation.} 
			\begin{equation*}
				trace(U AU^{-1}) = trace(AU^{-1}U) = trace(AI) = trace(A)
			\end{equation*}
			
			\item {Use question c) to
				prove that if matrix $ A $ of dimensions $ k \times k $ is diagonalizable then its trace is equal to the sum of its eigenvalues
				(actually this is true even if the matrix is not diagonalizable).}
			\begin{align*}
				\shortintertext{Because matrix $ A $ is diagonalizable, then there exists a matrix $ T $ such that:}
				\Lambda &= T^{-1}AT \\
				trace(\Lambda) &= trace(T^{-1}AT) = trace(A) \\
				\shortintertext{Notice that $ \Lambda $ is a diagonal matrix with all eigenvalues of $ A $ as its diagonal entries. Thus, } 
				trace(\Lambda) &= \text{sum of eigenvalues of $ A $.}  
			\end{align*}
			
			\item {Regarding question d) how do you explain
				this equality given that when $ A $ is real the trace is also real whereas the eigenvalues can be complex?} \\ \\
			Because the complex eigenvalues of $ A $ will always come in complex conjugate pairs. Therefore, when we sum them up, we will always get real numbers. \\
			
			\item {Using again question d) what can you say about the coefficient $ c_{k-1} $ of the characteristic polynomial
				$ \lambda^k + c_{k-1} \lambda^{k-1} + \dots + c_0 $ of $ A $. We recall that we already know that $ c_0 = (-1)^k \lambda_1 \cdots \lambda_k = (-1)^k \det(A) $.}
			\begin{align*}
				\det(\lambda I - A) &= \lambda^k + c_{k-1} \lambda^{k-1} + \dots + c_0 \\
				&= (\lambda - \lambda_1)(\lambda - \lambda_2) \dots (\lambda - \lambda_k) \\
				&= \lambda^k - \lambda^{k-1}(\lambda_1 + \lambda_2 + \dots + \lambda_k) + \cdots \\
				&= \lambda^k - trace(A)\lambda^{k-1} + \cdots 
				\shortintertext{Therefore, }
				c_{k-1} &= -trace(A)
			\end{align*}
		\end{subquestion}
		
		\newpage
		\item {A matrix $ A $ is called $ nilpotent $ if $ A^r = 0 $ for some integer $ r > 1 $.}
		\begin{subquestion}
			\item {Show that all eigenvalues of $ A $ must be equal to 0.}
			\begin{align*}
				\shortintertext{Take $ \vec{\bm{x}} $ to be an eigenvector of $ A $ associated with eigenvalue $ \lambda $, then}
				A \vec{\bm{x}} &= \lambda \vec{\bm{x}} \\
				A^2 \vec{\bm{x}} &= A (A\vec{\bm{x}}) = A (\lambda \vec{\bm{x}}) = \lambda^2 \vec{\bm{x}} \\
				\vdots \\
				A^r \vec{\bm{x}} &= \lambda^r \vec{\bm{x}} 
			\end{align*}
			From $ A^r = 0 $ we can get $ \lambda^r = 0 $. Thus, $ \lambda = 0 $. Note that all eigenvalues of $ A $ apply to the above equation. Therefore, all eigenvalues of $ A $ must be equal to 0. \\
			
			\item {Is such a matrix diagonalizable?}
			\begin{proof}
				Assume matrix $ A $ is diagonalizable, then there exits an invertible matrix $ T $ such that \[ A = T^{-1}\Lambda T. \] 
				The diagonal entries of matrix $ \Lambda $ are the eigenvalues of $ A $. Since all eigenvalues of $ A $ are equal to 0, $ \Lambda = 0 $. \\
				Thus, the right side of the equation equals to 0, while the left side does not. \\
				Therefore, no matrix $ T $ can satisfy this equation. This contradicts the assumption that there exits an invertible matrix $ T $ such that $ A = T^{-1}\Lambda T $. \\
				By contradiction, matrix $ A $ is non-diagonalizable.
			\end{proof}
			\item {Give an example of a $ 2 \times 2 $ matrix which is $ nilpotent $.}
			\begin{align*}
				\begin{bmatrix}
				0 & 1 \\
				0 & 0
				\end{bmatrix}
				\begin{bmatrix}
				0 & 1 \\
				0 & 0
				\end{bmatrix}
				=
				\begin{bmatrix}
				0 & 0 \\
				0 & 0
				\end{bmatrix}
				\shortintertext{Therefore, }
				\begin{bmatrix}
				0 & 1 \\
				0 & 0
				\end{bmatrix} && \text{is $ nilpotent $.}
			\end{align*}
			\item {If we apply a similarity transformation to a $ nilpotent $, is the resulting matrix $ nilpotent $? Apply the previous observation to your example in c) to obtain a second example of a $ nilpotent $ matrix.} \\
			\begin{align*}
				B &= T^{-1}A T \\
				B^2 &= T^{-1}A T T^{-1}A T = T^{-1}A^2 T \\
				\vdots \\
				B^r &= T^{-1}A^r T \\
				B^r &= 0 \\
				\shortintertext{Thus, if we apply a similarity transformation to a $ nilpotent $ $ A $, the resulting matrix $ B $ is $ nilpotent $.}
				A &= 
				\begin{bmatrix}
				0 & 1 \\
				0 & 0
				\end{bmatrix}
				\\
				T &= 
				\begin{bmatrix}
				0 & -1 \\
				1 & 0
				\end{bmatrix},
				T^{-1} =
				\begin{bmatrix}
				0 & 1 \\
				-1 & 0
				\end{bmatrix}
				\\
				B &= T^{-1}AT \\
				&= 
				\begin{bmatrix}
				0 & 1 \\
				-1 & 0
				\end{bmatrix}
				\begin{bmatrix}
				0 & 1 \\
				0 & 0
				\end{bmatrix}
				\begin{bmatrix}
				0 & -1 \\
				1 & 0
				\end{bmatrix}
				\\
				&= 
				\begin{bmatrix}
				0 & 0 \\
				0 & -1
				\end{bmatrix}
				\begin{bmatrix}
				0 & -1 \\
				1 & 0
				\end{bmatrix}
				\\
				&=
				\begin{bmatrix}
				0 & 0 \\
				-1 & 0
				\end{bmatrix}
				\\
				B^2 &= 
				\begin{bmatrix}
				0 & 0 \\
				-1 & 0
				\end{bmatrix}
				\begin{bmatrix}
				0 & 0 \\
				-1 & 0
				\end{bmatrix}
				\\
				&=
				\begin{bmatrix}
				0 & 0 \\
				0 & 0
				\end{bmatrix}
				\shortintertext{Thus, after rotating matrix $ A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} $ 90 degrees counterclockwisely, matrix $ B = \begin{bmatrix} 0 & 0 \\ -1 & 0 \end{bmatrix} $ is $ nilpotent $ for r = 2.}
			\end{align*}
		\end{subquestion}
	
		\item {Let $ Q $ be symmetric and positive definite with dimensions $ k \times k $. Denote with $ Q_i $ the matrix we obtain from $ Q $ by eliminating its $ i $th column and $ i $th row.}
		\begin{subquestion}
			\item {Show that $ Q_i $ is also symmetric and positive definite. Extend this to when we eliminate more than one columns and rows.}
			\begin{proof}
				Because $ Q $ is symmetric and positive definite. \\
				$ Q_{xy} = Q_{yx} $ for $ x, y \in [1, k] $. \\
				Thus, $ Q_{xy} = Q_{yx} $ is also true for $ x, y \in [1, i) \cup (i, k] $. \\
				Therefore, $ Q_i $ is symmetric. \\
				\\
				For any non-zero $ k \times 1 $ vector $ \vec{\bm{x}} $, we have $ \vec{\bm{x}}^T Q \vec{\bm{x}} > 0 $. \\
				Let $ \hat{\vec{\bm{x}}} $ be a vector we obtain from $ \vec{\bm{x}} $ by eliminating its $ i $th element. \\
				Let $ \hat{\hat{\vec{\bm{x}}}} = \vec{\bm{x}} $ except that $ \hat{\hat{\vec{\bm{x}}}}_i = 0 $, then we get $ \hat{\vec{\bm{x}}}^T Q_i \hat{\vec{\bm{x}}} = \hat{\hat{\vec{\bm{x}}}}^T Q \hat{\hat{\vec{\bm{x}}}} > 0 $. \\
				Therefore, $ Q_i $ is symmetric and positive definite.
			\end{proof}
		
			\item {Show that the largest
				eigenvalue of $ Q $ is larger than the largest eigenvalue of $ Q_i $ and, that the smallest eigenvalue of $ Q $ is smaller
				than the smallest eigenvalue of $ Q_i $ .}
			\begin{proof}
				\begin{align*}
					\lambda_{max}(Q_i) & = \max\limits_{\hat{\vec{\bm{x}}}} \frac{\hat{\vec{\bm{x}}}^T Q_i \hat{\vec{\bm{x}}}}{\hat{\vec{\bm{x}}}^T \hat{\vec{\bm{x}}}} \\
					\frac{\hat{\vec{\bm{x}}}^T Q_i \hat{\vec{\bm{x}}}}{\hat{\vec{\bm{x}}}^T \hat{\vec{\bm{x}}}} & = \frac{\hat{\hat{\vec{\bm{x}}}}^T Q \hat{\hat{\vec{\bm{x}}}}}{\hat{\hat{\vec{\bm{x}}}}^T \hat{\hat{\vec{\bm{x}}}}} \le \frac{\vec{\bm{x}}^T Q \vec{\bm{x}}}{\vec{\bm{x}}^T \vec{\bm{x}}} && \text{($ \hat{\hat{\vec{\bm{x}}}} = \vec{\bm{x}} $ except that $ \hat{\hat{\vec{\bm{x}}}}_i = 0 $)} \\
					\shortintertext{Therefore, }
					\min\limits_{\hat{\vec{\bm{x}}}} \frac{\hat{\vec{\bm{x}}}^T Q_i \hat{\vec{\bm{x}}}}{\hat{\vec{\bm{x}}}^T \hat{\vec{\bm{x}}}} & \le \max\limits_{\hat{\vec{\bm{x}}}} \frac{\hat{\vec{\bm{x}}}^T Q_i \hat{\vec{\bm{x}}}}{\hat{\vec{\bm{x}}}^T \hat{\vec{\bm{x}}}} \le 
					\min\limits_{\vec{\bm{x}}} \frac{\vec{\bm{x}}^T Q \vec{\bm{x}}}{\vec{\bm{x}}^T \vec{\bm{x}}} \le
					\max\limits_{\vec{\bm{x}}} \frac{\vec{\bm{x}}^T Q \vec{\bm{x}}}{\vec{\bm{x}}^T \vec{\bm{x}}} \\
					\lambda_{min}(Q_i) & \le\lambda_{max}(Q_i) \le \lambda_{min}(Q)\le \lambda_{max}(Q)
				\end{align*}
			\end{proof}
		
			\item {If $ A $ is a square matrix (not necessarily symmetric and positive
				definite) of dimensions $ k \times k $ and $ \lambda_1 , \dots , \lambda_k $ are its eigenvalues while $ \sigma_1 , \dots , \sigma_k $ are its singular values (obtained by applying SVD) then show \[ \min\limits_{i} \sigma_i \le \min\limits_{i} |\lambda i | \le \max\limits_{i} |\lambda i | \le \max\limits_{i} \sigma_i. \] In other words the eigenvalues are located on the complex plane inside an annulus with the two circle radii
				defined by the largest and smallest singular value. \textit{Use the result we proved in class regarding the bounds of
				the ratio $ \frac{\mathbf{x}^T C \mathbf{x}}{\mathbf{x}^T \mathbf{x}} $ for $ C $ symmetric. Consider that this is true even if $ \mathbf{x} $ is a vector with complex elements.}}
				\begin{proof}
					\begin{align*}
						\shortintertext{We know that:}
						\lambda_{max}(C) & \ge \frac{(\vec{\bm{x}}^*)^T C \vec{\bm{x}}}{(\vec{\bm{x}}^*)^T \vec{\bm{x}}} \ge \lambda_{min}(C) \\
						\sigma_1^2 & \ge \frac{(\vec{\bm{x}}^*)^T A^T A \vec{\bm{x}}}{(\vec{\bm{x}}^*)^T \vec{\bm{x}}} \ge \sigma_k^2 \\
						\shortintertext{From $ A\vec{\bm{x_{i}}} = \lambda_i\vec{\bm{x_{i}}} $ and $ A(\vec{\bm{x_{i}}}^*) = \lambda_i^*(\vec{\bm{x_{i}}}^*) $ we have:}
						\frac{(\vec{\bm{x_{i}}}^*)^T A^T A \vec{\bm{x_{i}}}}{(\vec{\bm{x_{i}}}^*)^T \vec{\bm{x_{i}}}} = \frac{(A\vec{\bm{x_{i}}}^*)^T A \vec{\bm{x_{i}}}}{(\vec{\bm{x_{i}}}^*)^T \vec{\bm{x_{i}}}} &= \frac{(\lambda_i^*(\vec{\bm{x_{i}}}^*))^T \lambda_i\vec{\bm{x_{i}}}}{(\vec{\bm{x_{i}}}^*)^T \vec{\bm{x_{i}}}} = \frac{\lambda_i^*(\vec{\bm{x_{i}}}^*)^T \lambda_i\vec{\bm{x_{i}}}}{(\vec{\bm{x}}^*)^T \vec{\bm{x}}} = \lambda_i^*\lambda_i = | \lambda_i |^2 \\
						\shortintertext{Thus,}
						\sigma_1^2 & \ge | \lambda_i |^2 \ge \sigma_k^2. \\
						\shortintertext{Because $ \sigma_k $ is not negative, }
						\sigma_1 & \ge | \lambda_i | \ge \sigma_k. \\
						\shortintertext{Therefore,}
						\min\limits_{i} \sigma_i \le \min\limits_{i} |\lambda i | & \le \max\limits_{i} |\lambda i | \le \max\limits_{i} \sigma_i.
					\end{align*}
				\end{proof}
		\end{subquestion}
	\end{enumerate}
\end{document}
