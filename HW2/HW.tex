\documentclass[letter, 12pt]{article}

\usepackage{amsmath,amsthm,amssymb}
\usepackage{fancyhdr}
\usepackage{geometry}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{eqparbox}
\usepackage{float}
\usepackage{bm}
\usepackage{mathtools}

\author{Shengjie Li}
\title{Homework 2}

\pagestyle{fancy}
\fancyhf{} 
\lhead{Shengjie Li \\ RUID: 188008047}
\cfoot{\thepage} 
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\headwidth}{\textwidth}
\renewcommand\algorithmiccomment[1]{%
	\hfill\#\ \eqparbox{COMMENT}{#1}%
}
\newlist{subquestion}{enumerate}{1}
\setlist[subquestion, 1]{label = \alph*)}

\setlength\parindent{0pt}

% margin adjustment
\addtolength{\textwidth}{1in}
\addtolength{\oddsidemargin}{-0.5in}
\addtolength{\evensidemargin}{-0.5in}
\addtolength{\topmargin}{-.5in}
\addtolength{\textheight}{1.0in}
\setlength\parindent{0cm}

\begin{document}
	\centerline{Homework 2}
	\begin{enumerate}[wide = 0pt, label = \textbf{Problem \arabic*:}]
		\item {Let $ \mathcal{X}_1 , \mathcal{X}_2 $ be two jointly Gaussian vectors with means $ \mu_1 , \mu_2 $ covariance matrices $ \Sigma_{11} , \Sigma_{22} $ and
			cross covariance matrix $ \Sigma_{12} = \mathbb{E}[(\mathcal{X}_1 - \mu_1 )(\mathcal{X}_2 - \mu_2 )^t ] $. By computing the conditional probability density
			prove that $ \mathcal{X}_1 $ given $ \mathcal{X}_2 $ continuous to be Gaussian with mean that depends on $ \mathcal{X}_2 $ but with a covariance
			matrix which is independent of $ \mathcal{X}_2 $ .}
		\begin{proof}
			\begin{align*}
				\Sigma &= \begin{bmatrix}
				\Sigma_{11} & \Sigma_{12} \\
				\Sigma_{21} &\Sigma_{22}
				\end{bmatrix} 
				= \mathbb{E}[\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix} \begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix}]
				\\
				\Sigma_{11} &= \mathbb{E}[\mathcal{X}_1, \mathcal{X}_1^t] \quad \quad
				\Sigma_{12} = \mathbb{E}[\mathcal{X}_1, \mathcal{X}_2^t] \\
				\Sigma_{21} &= \mathbb{E}[\mathcal{X}_2, \mathcal{X}_1^t] \quad \quad
				\Sigma_{22} = \mathbb{E}[\mathcal{X}_2, \mathcal{X}_2^t] 
				\shortintertext{Suppose $ \mathcal{X} $ has zero mean, we can get the joint probability density function}
				f(\mathcal{X}_1, \mathcal{X}_2) &= \frac{e^{-\frac{1}{2} \begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix} \Sigma^{-1} 
						\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix} }}{\sqrt{(2 \pi)^{d_1 + d_2}|\Sigma|}} \\
				f(\mathcal{X}_2) &= \frac{e^{-\frac{1}{2} \mathcal{X}_2^t \Sigma_{22}^t 
						\mathcal{X}_2 }}{\sqrt{(2 \pi)^{d_2}|\Sigma_{22}|}} \\
				f(\mathcal{X}_1 | \mathcal{X}_2) &= \frac{
					f(\mathcal{X}_1, \mathcal{X}_2)}{f(\mathcal{X}_2)} \\
				&= \frac{\frac{e^{-\frac{1}{2} \begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix} \Sigma^{-1} 
							\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix} }}{\sqrt{(2 \pi)^{d_1 + d_2}|\Sigma|}}}{\frac{e^{-\frac{1}{2} \mathcal{X}_2^t \Sigma_{22}^t 
							\mathcal{X}_2 }}{\sqrt{(2 \pi)^{d_2}|\Sigma_{22}|}}} \\
				&= \frac{e^{-\frac{1}{2} (\begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix} \Sigma^{-1} 
							\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix} - \mathcal{X}_2^t \Sigma_{22}^t 
							\mathcal{X}_2) }}{\sqrt{(2 \pi)^{d_1}\frac{|\Sigma|}{|\Sigma_{22}|}}}
				\shortintertext{By using Schur's Inversion Formula, we can get:} 
				\begin{bmatrix}
				\Sigma_{11} & \Sigma_{12} \\
				\Sigma_{21} &\Sigma_{22}
				\end{bmatrix} ^{-1}
				&=
				\begin{bmatrix}
				0 & 0 \\
				0 &\Sigma_{22}^{-1}
				\end{bmatrix} 
				+
				\begin{bmatrix}
				I \\ -E
				\end{bmatrix}
				\Delta^{-1}
				\begin{bmatrix}
				I & -F
				\end{bmatrix} \\
				E &= \Sigma_{22}^{-1} \Sigma_{21} = \Sigma_{22}^{-1} \Sigma_{12}^t \\
				F &= \Sigma_{12} \Sigma_{22}^{-1} \\
				\Delta &= \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1} \Sigma_{21} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1} \Sigma_{12}^t \\
				\begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix} \Sigma^{-1} 
				\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix} - \mathcal{X}_2^t \Sigma_{22}^t 
				\mathcal{X}_2
				&=
				\begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix} \begin{bmatrix}
				0 & 0 \\
				0 &\Sigma_{22}^{-1}
				\end{bmatrix} 
				\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix}
				+
				\begin{bmatrix} \mathcal{X}_1^t & \mathcal{X}_2^t \end{bmatrix} \begin{bmatrix}
				I \\ -E
				\end{bmatrix}
				\Delta^{-1}
				\begin{bmatrix}
				I & -F
				\end{bmatrix} 
				\begin{bmatrix} \mathcal{X}_1 \\ \mathcal{X}_2 \end{bmatrix}
				- 
				\mathcal{X}_2^t \Sigma_{22}^t 
				\mathcal{X}_2 \\
				&=
				(\mathcal{X}_1 - \Sigma_{12} \Sigma_{22}^{-1} \mathcal{X}_2)^t(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{12}^t)(\mathcal{X}_1 - \Sigma_{12} \Sigma_{22}^{-1} \mathcal{X}_2) \\
				\det
				\begin{bmatrix}
				\Sigma_{11} & \Sigma_{12} \\
				\Sigma_{11}^t &\Sigma_{22}
				\end{bmatrix}
				&=
				\det(\Sigma_{22})\det(\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^t) \\
				\frac{|\Sigma|}{|\Sigma_{22}|} &= |\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^t| \\
				f(\mathcal{X}_1 | \mathcal{X}_2) 
				&=
				\frac{e^{-\frac{1}{2}(\mathcal{X}_1 - \Sigma_{12} \Sigma_{22}^{-1} \mathcal{X}_2)^t(\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{12}^t)(\mathcal{X}_1 - \Sigma_{12} \Sigma_{22}^{-1} \mathcal{X}_2)}}{\sqrt{(2 \pi)^{d_1}|\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^t|}} \\
				\shortintertext{Thus,}
				\mathcal{X}_1 &\sim \mathcal{N}(\Sigma_{12}\Sigma_{22}^{-1}\mathcal{X}_2, \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^t) \\
				\shortintertext{When $ \mathcal{X}_1 $ and $ \mathcal{X}_2 $ do not have zero mean,}
				\mathcal{X}_1 - \mu_1 &\sim \mathcal{N}(\Sigma_{12}\Sigma_{22}^{-1}(\mathcal{X}_2 - \mu_2), \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}^t) \\
			\end{align*}
		\end{proof}
		
		\item {Consider a Bernoulli random variable $ \chi $ that takes the value $ a_1 $ with probability $ p $ and the
			value $ a_2 (a_2 \ne a_1 ) $ with probability $ 1 âˆ’p $. }
		\begin{subquestion}
			\item {Compute the the average and the variance of $ \chi $.}
			
			\item {Suppose now
				that you generate $ N $ independent realizations of $ \chi $. Propose a way to estimate $ p = \mathbb{P}(\chi = a_1 ) $.}
			
			\item {Compute
				the mean and variance of your estimate. What can you conclude from this computation when you consider
				$ N \rightarrow \infty $?}
		\end{subquestion}
	
		\item {$ \mathcal{X} $ is a random vector and there are $ K $ different possibilities that can generate realizations of this
			vector. Let $ f_1 (X), . . . , f_K (X) $ the corresponding pdfs and $ p_1 , . . . , p_K $ the corresponding prior probabilities
			that each case can occur of each possibility (with $ p_1 + \dots + p_K = 1 $). Using total probability and the trick
			that relates a pdf to the probability of a differential event, show that the pdf $ f (X) $ of $ \mathcal{X} $ satisfies \[ f_(X) = p_1 f_1 (X) + \dots + p_k f_K (X). \] Let now $ \chi_1 , \chi_2 $ be two random variables which 99\% of the time are independent and Normally (Gaussian)
			distributed, both with mean 0 and variance 1 and 1\% of the time they are independent and Normally
			distributed both with mean 0 and variance $ \sigma_2 \ne 1 $.}
		\begin{subquestion}
			\item {Compute the joint pdf of the two random variables.}
			
			\item {Examine if the two random variables are \textit{independent}.}
			
			\item {Give an example of two random variables that
				are \textit{uncorrelated} but not independent.}
		\end{subquestion}
	
		\item {Let $ \chi, \zeta $ be random variables that are related through the equality \[ \zeta = |\chi + s|. \]}
		\begin{subquestion}
			\item {If the pdf of $ \chi $ is $ f_\chi (x) $ compute the pdf of $ \zeta $ when $ s $ is a deterministic quantity.}
			
			\item {Repeat the previous
				question when $ s $ is a random variable independent from $ \chi $ and takes only the two values 0 and 1 with
				probabilities 0.2 and 0.8 respectively.}
			
			\item {Under the assumptions of question b) compute the posterior
				probability $ \mathbb{P}(s = 0|\zeta = z) $. \textit{Hint: For the computation of the pdf of a random variable the simplest way is
				to start with the computation of the cdf and then take the derivative. For b) use total probability.}}
		\end{subquestion}
		
		\item {Consider the space of all scalar random variables.}
		\begin{subquestion}
			\item {Show that this is a vector space by
				defining properly the operation of addition and multiplication.}
			
			\item {For any two random variables $ \chi, \psi $ we
				define the mapping $ < \chi, \psi > = \mathbb{E}[\chi\psi] $. Show that this mapping is an inner product in our vector space.}
			
			\item {What particular form do you obtain when you apply the general Schwarz inequality?}
			
			\item {How would you
				extend the previous definitions if you want a vector space comprised of \textit{random vectors} of length $ d $? Define
				properly the inner product and find the new form of the Schwartz inequality.}
			
		\end{subquestion}
	\end{enumerate}
\end{document}
